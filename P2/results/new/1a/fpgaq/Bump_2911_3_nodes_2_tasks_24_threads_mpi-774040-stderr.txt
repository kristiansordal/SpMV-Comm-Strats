srun: defined options
srun: -------------------- --------------------
srun: (null)              : n[017-018,020]
srun: cpus-per-task       : 24
srun: distribution        : block:block
srun: jobid               : 774040
srun: job-name            : Bump_2911_3_nodes_2_tasks_24_threads_mpi
srun: nodes               : 3
srun: ntasks              : 6
srun: ntasks-per-node     : 2
srun: verbose             : 1
srun: -------------------- --------------------
srun: end of defined options
srun: jobid 774040: nodes(3):`n[017-018,020]', cpu counts: 96(x3)
srun: Implicitly setting --exact, because -c/--cpus-per-task given.
srun: launch/slurm: launch_p_step_launch: CpuBindType=(null type)
srun: launching StepId=774040.0 on host n017, 2 tasks: [0-1]
srun: launching StepId=774040.0 on host n018, 2 tasks: [2-3]
srun: launching StepId=774040.0 on host n020, 2 tasks: [4-5]
srun: route/default: init: route default plugin loaded
srun: launch/slurm: _task_start: Node n017, 2 tasks started
srun: launch/slurm: _task_start: Node n020, 2 tasks started
srun: launch/slurm: _task_start: Node n018, 2 tasks started
[n020:1128679] common_ucx.c:174 using OPAL memory hooks as external events
[n020:1128679] pml_ucx.c:197 mca_pml_ucx_open: UCX version 1.15.0
[n020:1128680] common_ucx.c:174 using OPAL memory hooks as external events
[n020:1128680] pml_ucx.c:197 mca_pml_ucx_open: UCX version 1.15.0
[n018:827076] common_ucx.c:174 using OPAL memory hooks as external events
[n018:827076] pml_ucx.c:197 mca_pml_ucx_open: UCX version 1.15.0
[n017:2311386] common_ucx.c:174 using OPAL memory hooks as external events
[n017:2311386] pml_ucx.c:197 mca_pml_ucx_open: UCX version 1.15.0
[n018:827075] common_ucx.c:174 using OPAL memory hooks as external events
[n018:827075] pml_ucx.c:197 mca_pml_ucx_open: UCX version 1.15.0
[n017:2311385] common_ucx.c:174 using OPAL memory hooks as external events
[n017:2311385] pml_ucx.c:197 mca_pml_ucx_open: UCX version 1.15.0
[n020:1128679] common_ucx.c:332 self/memory: did not match transport list
[n020:1128680] common_ucx.c:332 self/memory: did not match transport list
[n020:1128680] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n020:1128680] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n020:1128680] common_ucx.c:337 support level is transports and devices
[n020:1128680] pml_ucx.c:289 mca_pml_ucx_init
[n020:1128679] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n020:1128679] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n020:1128679] common_ucx.c:337 support level is transports and devices
[n020:1128679] pml_ucx.c:289 mca_pml_ucx_init
[n017:2311386] common_ucx.c:332 self/memory: did not match transport list
[n017:2311386] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n017:2311386] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n017:2311386] common_ucx.c:337 support level is transports and devices
[n017:2311386] pml_ucx.c:289 mca_pml_ucx_init
[n020:1128679] pml_ucx.c:114 Pack remote worker address, size 303
[n020:1128679] pml_ucx.c:114 Pack local worker address, size 332
[n020:1128679] pml_ucx.c:351 created ucp context 0x55555594e4a0, worker 0x555555c22470
[n020:1128679] pml_ucx_component.c:147 returning priority 51
[n020:1128680] pml_ucx.c:114 Pack remote worker address, size 303
[n020:1128680] pml_ucx.c:114 Pack local worker address, size 332
[n020:1128680] pml_ucx.c:351 created ucp context 0x55555588bc00, worker 0x555555b155f0
[n020:1128680] pml_ucx_component.c:147 returning priority 51
[n017:2311385] common_ucx.c:332 self/memory: did not match transport list
[n017:2311385] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n017:2311385] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n017:2311385] common_ucx.c:337 support level is transports and devices
[n017:2311385] pml_ucx.c:289 mca_pml_ucx_init
[n018:827076] common_ucx.c:332 self/memory: did not match transport list
[n018:827076] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n018:827076] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n018:827076] common_ucx.c:337 support level is transports and devices
[n018:827076] pml_ucx.c:289 mca_pml_ucx_init
[n018:827075] common_ucx.c:332 self/memory: did not match transport list
[n018:827075] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n018:827075] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n018:827075] common_ucx.c:337 support level is transports and devices
[n018:827075] pml_ucx.c:289 mca_pml_ucx_init
[n017:2311386] pml_ucx.c:114 Pack remote worker address, size 303
[n017:2311386] pml_ucx.c:114 Pack local worker address, size 332
[n017:2311386] pml_ucx.c:351 created ucp context 0x55555593d740, worker 0x555555c21270
[n017:2311386] pml_ucx_component.c:147 returning priority 51
[n017:2311385] pml_ucx.c:114 Pack remote worker address, size 303
[n017:2311385] pml_ucx.c:114 Pack local worker address, size 332
[n017:2311385] pml_ucx.c:351 created ucp context 0x55555588a000, worker 0x555555c24050
[n017:2311385] pml_ucx_component.c:147 returning priority 51
[n018:827075] pml_ucx.c:114 Pack remote worker address, size 303
[n018:827075] pml_ucx.c:114 Pack local worker address, size 332
[n018:827075] pml_ucx.c:351 created ucp context 0x55555592fcb0, worker 0x555555c20050
[n018:827075] pml_ucx_component.c:147 returning priority 51
[n018:827076] pml_ucx.c:114 Pack remote worker address, size 303
[n018:827076] pml_ucx.c:114 Pack local worker address, size 332
[n018:827076] pml_ucx.c:351 created ucp context 0x5555559300c0, worker 0x555555c1ff60
[n018:827076] pml_ucx_component.c:147 returning priority 51
srun: Sent KVS info to 3 nodes, up to 2 tasks per node
[n017:2311386] common_ucx.c:332 self/memory: did not match transport list
[n017:2311386] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n017:2311386] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n017:2311386] common_ucx.c:337 support level is transports and devices
[n017:2311386] osc_ucx_component.c:222 returning priority 60
[n017:2311386] pml_ucx.c:182 Got proc 1 address, size 332
[n017:2311386] pml_ucx.c:416 connecting to proc. 1
[n020:1128680] common_ucx.c:332 self/memory: did not match transport list
[n020:1128680] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n020:1128680] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n020:1128680] common_ucx.c:337 support level is transports and devices
[n020:1128680] osc_ucx_component.c:222 returning priority 60
[n020:1128680] pml_ucx.c:182 Got proc 5 address, size 332
[n020:1128680] pml_ucx.c:416 connecting to proc. 5
[n017:2311386] pml_ucx.c:182 Got proc 0 address, size 332
[n017:2311386] pml_ucx.c:416 connecting to proc. 0
[n018:827076] common_ucx.c:332 self/memory: did not match transport list
[n018:827076] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n018:827076] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n018:827076] common_ucx.c:337 support level is transports and devices
[n018:827076] osc_ucx_component.c:222 returning priority 60
[n018:827076] pml_ucx.c:182 Got proc 3 address, size 332
[n018:827076] pml_ucx.c:416 connecting to proc. 3
[n017:2311385] common_ucx.c:332 self/memory: did not match transport list
[n017:2311385] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n017:2311385] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n017:2311385] common_ucx.c:337 support level is transports and devices
[n017:2311385] osc_ucx_component.c:222 returning priority 60
[n017:2311385] pml_ucx.c:182 Got proc 0 address, size 332
[n017:2311385] pml_ucx.c:416 connecting to proc. 0
[n018:827075] common_ucx.c:332 self/memory: did not match transport list
[n018:827075] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n018:827075] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n018:827075] common_ucx.c:337 support level is transports and devices
[n018:827075] osc_ucx_component.c:222 returning priority 60
[n018:827075] pml_ucx.c:182 Got proc 2 address, size 332
[n018:827075] pml_ucx.c:416 connecting to proc. 2
[n020:1128680] pml_ucx.c:182 Got proc 4 address, size 332
[n020:1128680] pml_ucx.c:416 connecting to proc. 4
[n018:827076] pml_ucx.c:182 Got proc 2 address, size 332
[n018:827076] pml_ucx.c:416 connecting to proc. 2
[n017:2311385] pml_ucx.c:182 Got proc 1 address, size 332
[n017:2311385] pml_ucx.c:416 connecting to proc. 1
[n018:827075] pml_ucx.c:182 Got proc 3 address, size 332
[n018:827075] pml_ucx.c:416 connecting to proc. 3
[n020:1128679] common_ucx.c:332 self/memory: did not match transport list
[n020:1128679] common_ucx.c:227 driver '../../../../bus/pci/drivers/mlx5_core' matched by 'mlx*'
[n020:1128679] common_ucx.c:323 rc_verbs/mlx5_0:1: matched both transport and device list
[n020:1128679] common_ucx.c:337 support level is transports and devices
[n020:1128679] osc_ucx_component.c:222 returning priority 60
[n020:1128679] pml_ucx.c:182 Got proc 4 address, size 332
[n020:1128679] pml_ucx.c:416 connecting to proc. 4
[n020:1128679] pml_ucx.c:182 Got proc 5 address, size 332
[n020:1128679] pml_ucx.c:416 connecting to proc. 5
srun: Sent KVS info to 3 nodes, up to 2 tasks per node
[n020:1128679] pml_ucx.c:182 Got proc 0 address, size 332
[n020:1128679] pml_ucx.c:416 connecting to proc. 0
[n020:1128680] pml_ucx.c:182 Got proc 0 address, size 332
[n020:1128680] pml_ucx.c:416 connecting to proc. 0
[n018:827076] pml_ucx.c:182 Got proc 0 address, size 332
[n018:827076] pml_ucx.c:416 connecting to proc. 0
[n018:827075] pml_ucx.c:182 Got proc 0 address, size 332
[n018:827075] pml_ucx.c:416 connecting to proc. 0
[n020:1128679] pml_ucx.c:928 ucx send nbr failed: 1, Operation in progress
[n020:1128679] *** An error occurred in MPI_Barrier
[n020:1128679] *** reported by process [3482845184,4]
[n020:1128679] *** on communicator MPI_COMM_WORLD
[n020:1128679] *** MPI_ERR_OTHER: known error not in list
[n020:1128679] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[n020:1128679] ***    and potentially your MPI job)
In: PMI_Abort(16, N/A)
1a: prov/util/src/util_mem_monitor.c:741: ofi_import_monitor_cleanup: Assertion `!impmon.impfid' failed.
[n020:1128679] *** Process received signal ***
[n020:1128679] Signal: Aborted (6)
[n020:1128679] Signal code:  (-6)
[n020:1128679] [ 0] /lib/x86_64-linux-gnu/libc.so.6(+0x42520)[0x7ffff78cb520]
[n020:1128679] [ 1] /lib/x86_64-linux-gnu/libc.so.6(pthread_kill+0x12c)[0x7ffff791f9fc]
[n020:1128679] [ 2] /lib/x86_64-linux-gnu/libc.so.6(raise+0x16)[0x7ffff78cb476]
[n020:1128679] [ 3] /lib/x86_64-linux-gnu/libc.so.6(abort+0xd3)[0x7ffff78b17f3]
[n020:1128679] [ 4] /lib/x86_64-linux-gnu/libc.so.6(+0x2871b)[0x7ffff78b171b]
[n020:1128679] [ 5] /lib/x86_64-linux-gnu/libc.so.6(+0x39e96)[0x7ffff78c2e96]
[n020:1128679] [ 6] /cm/shared/ex3-modules/202309a/defq/pkgs/libfabric-1.17.0/lib/libfabric.so.1(+0x6cf15)[0x7ffff67e4f15]
[n020:1128679] [ 7] /cm/shared/ex3-modules/202309a/defq/pkgs/libfabric-1.17.0/lib/libfabric.so.1(+0x6bc5e)[0x7ffff67e3c5e]
[n020:1128679] [ 8] /cm/shared/ex3-modules/202309a/defq/pkgs/libfabric-1.17.0/lib/libfabric.so.1(+0x17f14)[0x7ffff678ff14]
[n020:1128679] [ 9] /lib64/ld-linux-x86-64.so.2(+0x624e)[0x7ffff7fc924e]
[n020:1128679] [10] /lib/x86_64-linux-gnu/libc.so.6(+0x45495)[0x7ffff78ce495]
[n020:1128679] [11] /lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7ffff78ce610]
[n020:1128679] [12] /cm/shared/apps/slurm/current/lib64/libpmi.so.0(PMI_Abort+0x89)[0x7ffff6b06696]
[n020:1128679] [13] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/openmpi/mca_pmix_s1.so(+0x2c8d)[0x7ffff6b0dc8d]
[n020:1128679] [14] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/openmpi/mca_ess_pmi.so(+0x272a)[0x7ffff764072a]
[n020:1128679] [15] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libopen-rte.so.40(orte_errmgr_base_abort+0x12a)[0x7ffff782604a]
[n020:1128679] [16] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/openmpi/mca_errmgr_default_app.so(+0x15a3)[0x7ffff6c345a3]
[n020:1128679] [17] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(ompi_mpi_abort+0x30e)[0x7ffff7ed056e]
[n020:1128679] [18] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(ompi_mpi_errors_are_fatal_comm_handler+0xe3)[0x7ffff7ebf0c3]
[n020:1128679] [19] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(ompi_errhandler_invoke+0xd3)[0x7ffff7ebe3d3]
[n020:1128679] [20] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(MPI_Barrier+0x1a5)[0x7ffff7ee6a45]
[n020:1128679] [21] /home/krisor99/SpMV-Comm-Strats/P2/build/Debug/1a(+0x234c)[0x55555555634c]
[n020:1128679] [22] /lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7ffff78b2d90]
[n020:1128679] [23] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7ffff78b2e40]
srun: Complete StepId=774040.0+0 received
[n020:1128679] [24] /home/krisor99/SpMV-Comm-Strats/P2/build/Debug/1a(+0x2875)[0x555555556875]
[n020:1128679] *** End of error message ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
slurmstepd: error: *** STEP 774040.0 ON n017 CANCELLED AT 2025-04-01T13:05:54 ***
srun: Complete StepId=774040.0+0 received
[n020:1128680] pml_ucx.c:928 ucx send nbr failed: 1, Operation in progress
[n020:1128680] *** An error occurred in MPI_Barrier
[n020:1128680] *** reported by process [3482845184,5]
[n020:1128680] *** on communicator MPI_COMM_WORLD
[n020:1128680] *** MPI_ERR_OTHER: known error not in list
[n020:1128680] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[n020:1128680] ***    and potentially your MPI job)
In: PMI_Abort(16, N/A)
1a: prov/util/src/util_mem_monitor.c:741: ofi_import_monitor_cleanup: Assertion `!impmon.impfid' failed.
[n020:1128680] *** Process received signal ***
srun: Complete StepId=774040.0+0 received
[n020:1128680] Signal: Aborted (6)
[n020:1128680] Signal code:  (-6)
[n020:1128680] [ 0] /lib/x86_64-linux-gnu/libc.so.6(+0x42520)[0x7ffff78cb520]
[n020:1128680] [ 1] /lib/x86_64-linux-gnu/libc.so.6(pthread_kill+0x12c)[0x7ffff791f9fc]
[n020:1128680] [ 2] /lib/x86_64-linux-gnu/libc.so.6(raise+0x16)[0x7ffff78cb476]
[n020:1128680] [ 3] /lib/x86_64-linux-gnu/libc.so.6(abort+0xd3)[0x7ffff78b17f3]
[n020:1128680] [ 4] /lib/x86_64-linux-gnu/libc.so.6(+0x2871b)[0x7ffff78b171b]
[n020:1128680] [ 5] /lib/x86_64-linux-gnu/libc.so.6(+0x39e96)[0x7ffff78c2e96]
[n020:1128680] [ 6] /cm/shared/ex3-modules/202309a/defq/pkgs/libfabric-1.17.0/lib/libfabric.so.1(+0x6cf15)[0x7ffff67ebf15]
[n020:1128680] [ 7] /cm/shared/ex3-modules/202309a/defq/pkgs/libfabric-1.17.0/lib/libfabric.so.1(+0x6bc5e)[0x7ffff67eac5e]
[n020:1128680] [ 8] /cm/shared/ex3-modules/202309a/defq/pkgs/libfabric-1.17.0/lib/libfabric.so.1(+0x17f14)[0x7ffff6796f14]
[n020:1128680] [ 9] /lib64/ld-linux-x86-64.so.2(+0x624e)[0x7ffff7fc924e]
[n020:1128680] [10] /lib/x86_64-linux-gnu/libc.so.6(+0x45495)[0x7ffff78ce495]
[n020:1128680] [11] /lib/x86_64-linux-gnu/libc.so.6(on_exit+0x0)[0x7ffff78ce610]
[n020:1128680] [12] /cm/shared/apps/slurm/current/lib64/libpmi.so.0(PMI_Abort+0x89)[0x7ffff6b0d696]
[n020:1128680] [13] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/openmpi/mca_pmix_s1.so(+0x2c8d)[0x7ffff6b14c8d]
[n020:1128680] [14] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/openmpi/mca_ess_pmi.so(+0x272a)[0x7ffff764072a]
[n020:1128680] [15] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libopen-rte.so.40(orte_errmgr_base_abort+0x12a)[0x7ffff782604a]
[n020:1128680] [16] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/openmpi/mca_errmgr_default_app.so(+0x15a3)[0x7ffff6c345a3]
[n020:1128680] [17] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(ompi_mpi_abort+0x30e)[0x7ffff7ed056e]
[n020:1128680] [18] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(ompi_mpi_errors_are_fatal_comm_handler+0xe3)[0x7ffff7ebf0c3]
[n020:1128680] [19] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(ompi_errhandler_invoke+0xd3)[0x7ffff7ebe3d3]
[n020:1128680] [20] /cm/shared/ex3-modules/202309a/defq/pkgs/openmpi-4.1.6/lib/libmpi.so.40(MPI_Barrier+0x1a5)[0x7ffff7ee6a45]
[n020:1128680] [21] /home/krisor99/SpMV-Comm-Strats/P2/build/Debug/1a(+0x234c)[0x55555555634c]
[n020:1128680] [22] /lib/x86_64-linux-gnu/libc.so.6(+0x29d90)[0x7ffff78b2d90]
[n020:1128680] [23] /lib/x86_64-linux-gnu/libc.so.6(__libc_start_main+0x80)[0x7ffff78b2e40]
[n020:1128680] [24] /home/krisor99/SpMV-Comm-Strats/P2/build/Debug/1a(+0x2875)[0x555555556875]
[n020:1128680] *** End of error message ***
srun: launch/slurm: _task_finish: Received task exit notification for 2 tasks of StepId=774040.0 (status=0x0009).
srun: error: n017: tasks 0-1: Killed
srun: launch/slurm: _task_finish: Received task exit notification for 2 tasks of StepId=774040.0 (status=0x0009).
srun: error: n018: tasks 2-3: Killed
srun: launch/slurm: _task_finish: Received task exit notification for 2 tasks of StepId=774040.0 (status=0x0086).
srun: error: n020: tasks 4-5: Aborted (core dumped)
